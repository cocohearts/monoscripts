{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "47cd59de",
      "metadata": {
        "id": "47cd59de"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "from typing import Tuple, Union, Optional, List\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from typing import Callable\n",
        "from jaxtyping import Float, Int\n",
        "import einops\n",
        "from dataclasses import dataclass\n",
        "from torch.utils.data import DataLoader\n",
        "from accelerate import Accelerator\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "from accelerate import Accelerator, DistributedDataParallelKwargs, notebook_launcher\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "import ipywidgets as widgets\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c14370ec",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50257"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import dotenv\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "import os\n",
        "dotenv.load_dotenv()\n",
        "import huggingface_hub\n",
        "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
        "huggingface_hub.login(token=HUGGINGFACE_API_KEY)\n",
        "\n",
        "# ds = load_dataset(\"bigcode/the-stack-v2\", cache_dir=\"/shared/alex-zhao-storage/the-stack-v2\", split=\"train\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-11b-Vision\", cache_dir=\"/shared/alex-zhao-storage/hf-cache\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", cache_dir=\"/shared/alex-zhao-storage/hf-cache\")\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", cache_dir=\"/shared/alex-zhao-storage/hf-cache\")\n",
        "tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c91ac68b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def is_in_notebook():\n",
        "    try:\n",
        "        from IPython import get_ipython\n",
        "        if get_ipython() is not None and 'IPKernelApp' in get_ipython().config:\n",
        "            return True\n",
        "        return False\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "if is_in_notebook():\n",
        "    from tqdm.notebook import tqdm\n",
        "else:\n",
        "    from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "161ce944",
      "metadata": {},
      "outputs": [],
      "source": [
        "def num_params(model):\n",
        "    return sum(p.numel() for p in list(model.parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "137d2873",
      "metadata": {},
      "outputs": [],
      "source": [
        "debug = False\n",
        "if debug:\n",
        "    print(tokenizer.decode(tokenizer.encode(\"hello there, happy world! test lol lol\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9d17a00a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ds = load_dataset(\"HuggingFaceFW/fineweb-edu\", \"default\")\n",
        "# L = torch.load('/shared/alex-zhao-storage/tiny-textbook-ds.pt')\n",
        "# dataloader = DataLoader(L['input_ids'], batch_size=32, num_workers=8)\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text, tokenizer):\n",
        "        self.text = text\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        output = self.tokenizer.encode(self.text[idx], return_tensors=\"pt\", padding=True, truncation=True, padding_side=\"left\", max_length=512)\n",
        "        # Squeeze to remove batch dimension added by tokenizer\n",
        "        output = output.squeeze(0)\n",
        "\n",
        "        # Pad to length 513 from the left if needed\n",
        "        padding_length = 512 - output.size(0)\n",
        "        padding = torch.full((padding_length,), self.tokenizer.pad_token_id)\n",
        "        output = torch.cat([padding, output], dim=0)\n",
        "            \n",
        "        return output\n",
        "    \n",
        "def get_dataloader(batch_size=16):\n",
        "    # tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", cache_dir=\"/shared/alex-zhao-storage/hf-cache\")\n",
        "    # dataset = load_dataset(\"nampdn-ai/tiny-textbooks\", cache_dir=\"/shared/alex-zhao-storage/hf-cache\")\n",
        "    dataset = load_dataset(\"DKYoon/SlimPajama-6B\", cache_dir=\"/shared/alex-zhao-storage/hf-cache\", split=\"train\")\n",
        "    return DataLoader(TextDataset(dataset['text'], tokenizer), batch_size=batch_size, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3450e5ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "if debug:\n",
        "    dataloader = get_dataloader()\n",
        "    for batch in dataloader:\n",
        "        print(batch.shape)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "baab5229",
      "metadata": {
        "id": "baab5229"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50257"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88556b9a",
      "metadata": {},
      "source": [
        "# Prefix Ops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5646f2f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "from math import log2, ceil\n",
        "class PrefixOps():\n",
        "    # Assumes that position is a power of 2\n",
        "    def pref_mul(t: Float[Tensor, \"batch position d_model d_state\"]):\n",
        "        n_layers = int(log2(t.shape[1]))\n",
        "        up_tensors = []\n",
        "        down_tensors = []\n",
        "        up_tensors.append(t)\n",
        "        for _ in range(n_layers):\n",
        "            left = up_tensors[-1][:, ::2]\n",
        "            right = up_tensors[-1][:, 1::2]\n",
        "            up_tensors.append(left * right)\n",
        "\n",
        "        down_tensors.append(torch.ones_like(up_tensors[-1]))\n",
        "        for index in range(n_layers):\n",
        "            new = torch.zeros_like(up_tensors[-2 - index])\n",
        "            new[:, ::2] = down_tensors[-1]\n",
        "            new[:, 1::2] = down_tensors[-1] * up_tensors[-2 - index][:, ::2]\n",
        "            down_tensors.append(new)\n",
        "        return down_tensors[-1] * t\n",
        "\n",
        "    def pref_add(t: Float[Tensor, \"batch position d_model d_state\"]):\n",
        "        n_layers = ceil(log2(t.shape[1]))\n",
        "        len_diff = 2**n_layers - t.shape[1]\n",
        "        t = torch.cat([t, torch.zeros(t.shape[0], len_diff, t.shape[2], t.shape[3]).to(t.device)], dim=1)\n",
        "        up_tensors = []\n",
        "        down_tensors = []\n",
        "        up_tensors.append(t)\n",
        "        for _ in range(n_layers):\n",
        "            left = up_tensors[-1][:, ::2]\n",
        "            right = up_tensors[-1][:, 1::2]\n",
        "            up_tensors.append(left + right)\n",
        "\n",
        "        down_tensors.append(torch.zeros_like(up_tensors[-1]))\n",
        "        for index in range(n_layers):\n",
        "            new = torch.zeros_like(up_tensors[-2 - index])\n",
        "            new[:, ::2] = down_tensors[-1]\n",
        "            new[:, 1::2] = down_tensors[-1] + up_tensors[-2 - index][:, ::2]\n",
        "            down_tensors.append(new)\n",
        "        output = down_tensors[-1] + t\n",
        "        return output[:, :t.shape[1] - len_diff]\n",
        "\n",
        "class TestPrefixOps():\n",
        "    def __init__(self, batch, position, d_model, d_state):\n",
        "        self.batch = batch\n",
        "        self.position = position\n",
        "        self.d_model = d_model\n",
        "        self.d_state = d_state\n",
        "\n",
        "    def pref_mul(self):\n",
        "        my_in = torch.exp(torch.randn(self.batch, self.position, self.d_model, self.d_state))\n",
        "        my_out = PrefixOps.pref_mul(my_in)\n",
        "        true_out = torch.ones_like(my_in)\n",
        "        for i in range(self.position):\n",
        "            if i==0:\n",
        "                true_out[:, i] = my_in[:, i]\n",
        "            else:\n",
        "                true_out[:, i] = true_out[:, i-1] * my_in[:, i]\n",
        "        assert(torch.allclose(my_out, true_out))\n",
        "    \n",
        "    def pref_add(self):\n",
        "        my_in = torch.randn(self.batch, self.position, self.d_model, self.d_state)\n",
        "        my_out = PrefixOps.pref_add(my_in)\n",
        "        true_out = torch.zeros_like(my_in)\n",
        "        for i in range(self.position):\n",
        "            if i==0:\n",
        "                true_out[:, i] = my_in[:, i]\n",
        "            else:\n",
        "                true_out[:, i] = true_out[:, i-1] + my_in[:, i]\n",
        "        assert(torch.allclose(my_out, true_out, atol=1e-4))\n",
        "\n",
        "TestPrefixOps(12, 64, 768, 64).pref_mul()\n",
        "TestPrefixOps(12, 64, 768, 64).pref_add()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6eb743ee",
      "metadata": {
        "id": "6eb743ee"
      },
      "source": [
        "# Implement Mamba"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6cffe0a",
      "metadata": {
        "id": "e6cffe0a"
      },
      "source": [
        "## Single Head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "5ZVRz_LHQNHo",
      "metadata": {
        "id": "5ZVRz_LHQNHo"
      },
      "outputs": [],
      "source": [
        "test_ssm_ablation = False\n",
        "pref_sum = True\n",
        "use_double = False\n",
        "careful_double = True\n",
        "\n",
        "class SSM(nn.Module):\n",
        "    def __init__(self, d_model, d_state):\n",
        "        super().__init__()\n",
        "\n",
        "        # get A by negative softplus\n",
        "        self.Araw = nn.Parameter(torch.randn(d_model, d_state))\n",
        "        torch.nn.init.kaiming_normal_(self.Araw)\n",
        "        self.Araw.data = self.Araw.data.clamp(min=-4)\n",
        "        self.s_B = nn.Linear(d_model, d_state)\n",
        "        self.s_C = nn.Linear(d_model, d_state)\n",
        "        self.s_D = nn.Linear(d_model, 1)\n",
        "        self.P = nn.Parameter(torch.randn(d_model))\n",
        "        # torch.nn.init.xavier_normal_(self.P)\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_state = d_state\n",
        "\n",
        "    def forward(self, x: Float[Tensor, \"batch position d_model\"], keep_hidden=False, use_hidden=False) -> Float[Tensor, \"batch position d_model\"]:\n",
        "        sp = nn.Softplus()\n",
        "        disc = sp(self.P + self.s_D(x).repeat(1, 1, self.d_model)) # size batch position d_model\n",
        "        A = -1 * sp(self.Araw)\n",
        "        A_bar_pre = disc[..., None] * A # size batch position d_model d_state\n",
        "        # A_bar_pre = torch.where(torch.abs(A_bar_pre) < 1e-4, -1e-4 * torch.ones_like(A_bar_pre), A_bar_pre)\n",
        "        A_bar = torch.exp(A_bar_pre)\n",
        "        B = self.s_B(x) # size batch position d_state\n",
        "        # ratio = 1 + A_bar_pre/2 + A_bar_pre**2/2 + A_bar_pre**3/6 + A_bar_pre**4/24 + A_bar_pre**5/120\n",
        "        B_bar = B[:, :, None] / A * (A_bar - 1)\n",
        "\n",
        "        C = self.s_C(x)\n",
        "\n",
        "        assert(str(x.device)[:4] == 'cuda')\n",
        "\n",
        "        if pref_sum and not use_hidden:\n",
        "            Bx = B_bar * x[..., None]\n",
        "            Bx_log = torch.log(torch.abs(Bx))\n",
        "            A_bar_prod_log = PrefixOps.pref_add(A_bar_pre)\n",
        "            # AinvsB_log must be at most 77\n",
        "            if use_double:\n",
        "                A_bar_prod_log = torch.max(A_bar_prod_log, Bx_log-150)\n",
        "                if careful_double:\n",
        "                    A_bar_prod_log = torch.max(A_bar_prod_log, Bx_log-70)\n",
        "                AinvsB_log = Bx_log - A_bar_prod_log\n",
        "                AinvsBsum = PrefixOps.pref_add(torch.exp(AinvsB_log.to(torch.double)) * torch.sign(Bx))\n",
        "                assert(AinvsBsum.dtype == torch.double)\n",
        "                y = torch.matmul((AinvsBsum * torch.exp(A_bar_prod_log.to(torch.double))).to(torch.float), C[..., None]).squeeze(-1).to(x.dtype)\n",
        "            else:\n",
        "                A_bar_prod_log = torch.max(A_bar_prod_log, Bx_log-70)\n",
        "                AinvsB_log = Bx_log - A_bar_prod_log\n",
        "                AinvsBsum = PrefixOps.pref_add(torch.exp(AinvsB_log) * torch.sign(Bx))\n",
        "                h = AinvsBsum * torch.exp(A_bar_prod_log)\n",
        "                y = torch.matmul(h, C[..., None]).squeeze(-1)\n",
        "            if torch.isnan(y).any() or torch.isinf(y).any():\n",
        "                raise ValueError(\"NaN or Inf values detected in SSM output\")\n",
        "        else:\n",
        "            if use_hidden:\n",
        "                h = self.h\n",
        "            else:\n",
        "                h = torch.zeros(x.shape[0], self.d_model, self.d_state).to(x.device)\n",
        "            y = torch.zeros_like(x).to(x.device)\n",
        "            for index in range(x.shape[1]):\n",
        "                if index == 0:\n",
        "                    h = B_bar[:, index] * x[:, index].view(-1, self.d_model, 1)\n",
        "                else:\n",
        "                    h = A_bar[:, index] * h + B_bar[:, index] * x[:, index].view(-1, self.d_model, 1)\n",
        "                y[:, index] = torch.matmul(h, C[:, index, :, None]).squeeze(-1)\n",
        "\n",
        "        if torch.isnan(y).any() or torch.isinf(y).any():\n",
        "            raise ValueError(\"NaN or Inf values detected in SSM output\")\n",
        "        if keep_hidden:\n",
        "            self.h = h[:, -1:]\n",
        "        return y\n",
        "\n",
        "    def device(self):\n",
        "        return next(self.parameters()).device\n",
        "    \n",
        "    def test_forward(self, x: Float[Tensor, \"batch position d_model\"]):\n",
        "        sp = nn.Softplus()\n",
        "        disc = sp(self.P + self.s_D(x).repeat(1, 1, self.d_model)) # size batch position d_model\n",
        "        A_bar_pre = disc[..., None] * (-1 * sp(self.Araw)) # size batch position d_model d_state\n",
        "        # A_bar_pre = torch.where(torch.abs(A_bar_pre) < 1e-4, -1e-4 * torch.ones_like(A_bar_pre), A_bar_pre)\n",
        "        A_bar = torch.exp(A_bar_pre)\n",
        "        B = self.s_B(x) # size batch position d_state\n",
        "        ratio = 1 + A_bar_pre/2 + A_bar_pre**2/2 + A_bar_pre**3/6 + A_bar_pre**4/24 + A_bar_pre**5/120\n",
        "        B_bar = ratio * (torch.unsqueeze(disc, -1) * torch.unsqueeze(B, 2)) # size batch position d_model d_state\n",
        "\n",
        "        C = self.s_C(x)\n",
        "\n",
        "        assert(str(x.device)[:4] == 'cuda')\n",
        "        h = torch.zeros(x.shape[0], self.d_model, self.d_state).to(x.device)\n",
        "\n",
        "        test_y = self.forward(x)\n",
        "\n",
        "        y = torch.zeros_like(x).to(x.device)\n",
        "        for index in range(x.shape[1]):\n",
        "            if index == 0:\n",
        "                h = B_bar[:, index] * x[:, index].view(-1, self.d_model, 1)\n",
        "            else:\n",
        "                h = A_bar[:, index] * h + B_bar[:, index] * x[:, index].view(-1, self.d_model, 1)\n",
        "            y[:, index] = torch.matmul(h, C[:, index, :, None]).squeeze(-1)\n",
        "        print(\"top 20 diff: \", (y / test_y).abs().flatten().topk(50).values)\n",
        "        twox_diff = (y / test_y).abs() > 2\n",
        "        print(\">2x diff: \", twox_diff.sum().item())\n",
        "        sixx_diff = (y / test_y).abs() > 6\n",
        "        print(\">6x diff: \", sixx_diff.sum().item())\n",
        "        print(\"6x diff output vals: \", test_y[sixx_diff].flatten())\n",
        "\n",
        "    # def inf_forward(self, x: Float[Tensor, \"d_model\"]) -> Float[Tensor, \"d_model\"]:\n",
        "    #     sp = nn.Softplus()\n",
        "    #     disc = sp(self.P + self.s_D(x).repeat(1, 1, self.d_model)) # size batch position d_model\n",
        "    #     A_bar_pre = torch.unsqueeze(disc, -1) * self.A # size batch position d_model d_state\n",
        "    #     A_bar = torch.exp(A_bar_pre)\n",
        "    #     B = self.s_B(x) # size batch position d_state\n",
        "    #     B_bar = (A_bar - 1) / A_bar_pre * (torch.unsqueeze(disc, -1) * torch.unsqueeze(B, 2)) # size batch position d_model d_state\n",
        "\n",
        "    #     C = self.s_C(x)\n",
        "\n",
        "    #     self.h = A_bar * self.h + B_bar * x\n",
        "    #     return torch.matmul(self.h, C[:, None])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "29fc4254",
      "metadata": {
        "id": "29fc4254"
      },
      "outputs": [],
      "source": [
        "class Mamba(nn.Module):\n",
        "    def __init__(self, d_model, d_state, d_conv, expand):\n",
        "        # dim: the dimension of the input\n",
        "        # n_hidden: the dimension of the keys, queries, and values\n",
        "\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_state = d_state\n",
        "        self.d_conv = d_conv\n",
        "        self.expand = expand\n",
        "\n",
        "        d_head = expand * d_state\n",
        "        self.upscale = nn.Linear(d_model, d_head)\n",
        "        self.gate = nn.Linear(d_model, d_head)\n",
        "        self.conv = nn.Conv1d(d_head, d_head, d_conv, padding=d_conv-1, groups=d_head)\n",
        "        self.ssm = SSM(d_head, d_state)\n",
        "        self.downscale = nn.Linear(d_head, d_model)\n",
        "\n",
        "        self.silu = nn.SiLU()\n",
        "\n",
        "    def forward(self, x: Float[Tensor, \"batch position d_model\"], keep_hidden=False, use_hidden=False) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        upscaled = self.upscale(x)\n",
        "        conv_out = self.conv(upscaled.transpose(1,2)).transpose(1,2)[:, :upscaled.shape[1]]\n",
        "        ssm_out = self.ssm(self.silu(conv_out), keep_hidden=keep_hidden, use_hidden=use_hidden)\n",
        "        gate_output = self.silu(self.gate(x))\n",
        "        final_output = self.downscale(ssm_out * gate_output)\n",
        "\n",
        "        if torch.isnan(final_output).any() or torch.isinf(final_output).any():\n",
        "            raise ValueError(\"NaN or Inf values detected in Mamba output\")\n",
        "\n",
        "        return final_output\n",
        "    \n",
        "    def generate(self, x: Float[Tensor, \"batch position d_model\"], new_tokens: int):\n",
        "        return self.forward(self.generate(x, new_tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ddd56734",
      "metadata": {
        "id": "ddd56734"
      },
      "outputs": [],
      "source": [
        "from einops import rearrange\n",
        "\n",
        "class MambaLayer(nn.Module):\n",
        "    def __init__(self, n_heads, d_model, d_state, d_conv, expand):\n",
        "        super().__init__()\n",
        "\n",
        "        self.Heads = nn.ModuleList([Mamba(d_model, d_state, d_conv, expand) for _ in range(n_heads)])\n",
        "        self.n_heads = n_heads\n",
        "        self.rms_norm = nn.RMSNorm((d_model))\n",
        "        self.out_project = nn.Linear(d_model, d_model)\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x: Float[Tensor, \"batch position d_model\"], keep_hidden=False, use_hidden=False) -> Float[Tensor, \"batch position d_model\"]:\n",
        "        if torch.isinf(x).any():\n",
        "            raise ValueError(\"Inf values detected in MambaLayer input\")\n",
        "        if torch.isnan(x).any():\n",
        "            raise ValueError(\"NaN values detected in MambaLayer input\")\n",
        "        normed_x = self.rms_norm(x)\n",
        "        head_outputs = torch.zeros(x.shape).to(x.device)\n",
        "        for head in self.Heads:\n",
        "            head_outputs += head(normed_x, keep_hidden=keep_hidden, use_hidden=use_hidden)\n",
        "        x = x + self.layer_norm(self.out_project(head_outputs))\n",
        "        if torch.isnan(x).any() or torch.isinf(x).any():\n",
        "            raise ValueError(\"NaN or Inf values detected in MambaLayer forward\")\n",
        "        return x\n",
        "    \n",
        "    def generate(self, x: Float[Tensor, \"batch position d_model\"], new_tokens: int):\n",
        "        normed_x = self.rms_norm(x)\n",
        "        \n",
        "        out = self.forward(normed_x, keep_hidden=True)\n",
        "        head_outputs = torch.zeros(x.shape).to(x.device)\n",
        "        for head in self.Heads:\n",
        "            head_outputs += head.generate(normed_x, new_tokens)\n",
        "        x = x + self.layer_norm(self.out_project(head_outputs))\n",
        "        if torch.isnan(x).any() or torch.isinf(x).any():\n",
        "            raise ValueError(\"NaN or Inf values detected in MambaLayer forward\")\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "vI6qu8MVqVbt",
      "metadata": {
        "id": "vI6qu8MVqVbt"
      },
      "outputs": [],
      "source": [
        "class FFN(nn.Module):\n",
        "    def __init__(self, dim: int, n_hidden: int):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, n_hidden),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(n_hidden, dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor)-> torch.Tensor:\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ueCWqzJdqXlg",
      "metadata": {
        "id": "ueCWqzJdqXlg"
      },
      "outputs": [],
      "source": [
        "class MambaLM(nn.Module):\n",
        "    def __init__(self, vocab_size, n_layers, n_heads, d_model, d_state, d_conv, expand, context_len=1000):\n",
        "        super().__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.n_layers = n_layers\n",
        "        self.n_heads = n_heads\n",
        "        self.d_model = d_model\n",
        "        self.d_state = d_state\n",
        "        self.d_conv = d_conv\n",
        "        self.expand = expand\n",
        "        self.context_len = context_len\n",
        "\n",
        "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
        "\n",
        "        self.pos_embedding = nn.Embedding(self.context_len, d_model)\n",
        "\n",
        "        self.pos_embedding.weight.data[:, ::2] = torch.sin(torch.arange(0, self.context_len)[:, None] / 10000 ** (torch.arange(0, self.d_model, 2)[None, :] / self.d_model))\n",
        "        self.pos_embedding.weight.data[:, 1::2] = torch.cos(torch.arange(0, self.context_len)[:, None] / 10000 ** (torch.arange(1, self.d_model, 2)[None, :] / self.d_model))\n",
        "\n",
        "        self.layers = nn.ModuleList([MambaLayer(n_heads, d_model, d_state, d_conv, expand) for _ in range(n_layers)])\n",
        "        self.output_layer = nn.Linear(d_model, self.vocab_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, keep_hidden=False, use_hidden=False, position_shift=0) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # Count zeros on left side of each sequence\n",
        "        mask = (x == 0).to(self.device())\n",
        "        left_zeros = mask.cummin(dim=1)[0].sum(dim=1, keepdim=True)\n",
        "\n",
        "        pos_embed_indices = torch.arange(x.shape[1]).expand(x.shape[0], -1).to(self.device()) - left_zeros\n",
        "        pos_embed_indices = torch.where(pos_embed_indices >= 0, pos_embed_indices, 0)\n",
        "        pos_embed_indices += position_shift\n",
        "        pos_embed = torch.where(pos_embed_indices.unsqueeze(-1) >= 0, self.pos_embedding(pos_embed_indices), 0)\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x = x + pos_embed\n",
        "        if x.isnan().any() or x.isinf().any():\n",
        "            raise ValueError(\"NaN or Inf values detected in MambaLM input\")\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, keep_hidden=keep_hidden, use_hidden=use_hidden)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "    \n",
        "    @staticmethod\n",
        "    def train_step(model, x: Int[Tensor, \"batch position\"], optimizer, scheduler, accelerator):\n",
        "        optimizer.zero_grad()\n",
        "        out = model.forward(x[:, :-1])\n",
        "        ce = nn.CrossEntropyLoss()\n",
        "        loss = ce(out.transpose(1,2), x[:, 1:])\n",
        "        if accelerator is not None:\n",
        "            accelerator.backward(loss)\n",
        "        else:\n",
        "            loss.backward()\n",
        "        \n",
        "        # Check for nan gradients and zero them out\n",
        "        for param in model.parameters():\n",
        "            if param.grad is not None and (torch.isnan(param.grad).any() or torch.isinf(param.grad).any()):\n",
        "                param.grad.zero_()\n",
        "                # print(f\"Gradient for {param.name} with {param.numel()} elements is {'nan' if torch.isnan(param.grad).any() else 'inf'}. Zeroing out.\")\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # clip the gradients\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        return loss\n",
        "    \n",
        "    def device(self):\n",
        "        return next(self.parameters()).device\n",
        "\n",
        "    @staticmethod\n",
        "    def train(model, ds, optimizer, scheduler, epochs=1, accelerator=None):\n",
        "        best_loss = float('inf')\n",
        "        is_main_process = accelerator is not None and accelerator.is_main_process\n",
        "        progress_bar = tqdm(range(epochs), \n",
        "                           desc=f\"Training (process {accelerator.process_index})\", \n",
        "                           position=accelerator.process_index*2,\n",
        "                           leave=True,\n",
        "                           disable=not is_main_process)\n",
        "        batch_progress = tqdm(ds, \n",
        "                             desc=f\"Batches (process {accelerator.process_index})\", \n",
        "                             position=accelerator.process_index*2+1,\n",
        "                             leave=True,\n",
        "                             disable=not is_main_process)\n",
        "            \n",
        "        for _ in progress_bar:\n",
        "            epoch_loss = 0\n",
        "            batch_progress.reset()\n",
        "\n",
        "            for index, batch in enumerate(batch_progress):\n",
        "                # if accelerator is not None and accelerator.is_main_process:\n",
        "                loss = MambaLM.train_step(model, batch.to(next(model.parameters()).device), optimizer, scheduler, accelerator)\n",
        "                epoch_loss += loss.item()\n",
        "                batch_progress.update(1)\n",
        "                batch_progress.set_description(f\"Loss: {epoch_loss / (index + 1):0.4f}\")\n",
        "\n",
        "                if index % 100 == 0 and is_main_process:\n",
        "                    print(f\"Test generation `the weather today is` at {index / len(ds) * 100:.2f}% completion: \", model.generate_text(\"the weather today is\", 10))\n",
        "\n",
        "            if epoch_loss < best_loss:\n",
        "                best_loss = epoch_loss\n",
        "                if accelerator is not None and accelerator.is_main_process:\n",
        "                    torch.save(model.state_dict(), \"mamba_lm.pt\")\n",
        "\n",
        "        if is_main_process:\n",
        "            print(f\"Best loss: {best_loss:0.4f}\")\n",
        "                    \n",
        "            \n",
        "    def generate(self, x: Int[Tensor, \"batch position\"], new_tokens: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        returned = x\n",
        "\n",
        "        out = self.forward(x, keep_hidden=True)\n",
        "        next_token = out[:, -1].argmax(dim=-1)[None, :]\n",
        "        returned = torch.cat([returned, next_token], dim=1)\n",
        "\n",
        "        for _ in range(new_tokens-1):\n",
        "            out = self.forward(next_token, keep_hidden=True, use_hidden=True)\n",
        "            next_token = out[:, -1].argmax(dim=-1)[None, :]\n",
        "            returned = torch.cat([returned, next_token], dim=1)\n",
        "        return returned\n",
        "\n",
        "    def generate_text(self, text, new_tokens):\n",
        "        tokens = tokenizer.encode(text)\n",
        "        tokens = torch.tensor(tokens).unsqueeze(0).to(self.device())\n",
        "        return tokenizer.decode(self.generate(tokens, new_tokens)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3096185a",
      "metadata": {
        "id": "3096185a"
      },
      "source": [
        "# Get Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "13a6fe8d",
      "metadata": {
        "id": "13a6fe8d"
      },
      "outputs": [],
      "source": [
        "# import wget\n",
        "# import os\n",
        "# if not os.path.exists(\"input.txt\"):\n",
        "#     wget.download(\"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\")\n",
        "\n",
        "# with open('input.txt', 'r') as f:\n",
        "#     raw_text = f.read()\n",
        "# all_dialogues = raw_text.split('\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "5fe66396",
      "metadata": {},
      "outputs": [],
      "source": [
        "if debug:\n",
        "    ds = load_dataset(\"nampdn-ai/tiny-textbooks\", cache_dir=\"/shared/alex-zhao-storage/hf-cache\")\n",
        "    print(tokenizer.encode(\"hello there, happy world!\"))\n",
        "    lengths = [len(text) for text in ds['train']['textbook']]\n",
        "    print(f\"Number of texts: {len(lengths)}\")\n",
        "    print(f\"Mean length: {np.mean(lengths):.1f}\")\n",
        "    print(f\"Std length: {np.std(lengths):.1f}\") \n",
        "    print(f\"Min length: {min(lengths)}\")\n",
        "    print(f\"Max length: {max(lengths)}\")\n",
        "    print(f\"Median length: {np.median(lengths):.1f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "d3cd428c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# L = torch.load('/shared/alex-zhao-storage/tiny-textbook-ds.pt')\n",
        "# special_print(f\"Loaded dataset with {len(L['input_ids'])} samples\", accelerator)\n",
        "# dataloader = DataLoader(L['input_ids'], batch_size=1, num_workers=0)\n",
        "# special_print(f\"Loaded dataloader with {len(dataloader)} batches\", accelerator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "cd3f86cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "def special_print(my_str, accelerator):\n",
        "    if accelerator.is_main_process:\n",
        "        print(my_str)\n",
        "        print_time()\n",
        "\n",
        "def print_time():\n",
        "    from datetime import datetime\n",
        "    now = datetime.now()\n",
        "    current_time = now.strftime(\"%H:%M:%S.%f\")[:-3]\n",
        "    print(\"Current Time =\", current_time)\n",
        "\n",
        "vocab_size = tokenizer.vocab_size\n",
        "\n",
        "def training_function():\n",
        "    accelerator = Accelerator()\n",
        "    special_print(\"Accelerator initialized\", accelerator)\n",
        "\n",
        "    dataloader = get_dataloader(4)\n",
        "    special_print(f\"Loaded dataloader with {len(dataloader)} batches\", accelerator)\n",
        "    for batch in dataloader:\n",
        "        tokens_per_batch = batch.numel()\n",
        "        break\n",
        "    num_tokens = len(dataloader) * tokens_per_batch\n",
        "    print(f\"Number of tokens: {num_tokens}\")\n",
        "    \n",
        "    # model = MambaLM(vocab_size, \n",
        "    #             n_layers=12,\n",
        "    #             n_heads=6,\n",
        "    #             d_model=192,\n",
        "    #             d_state=32,\n",
        "    #             d_conv=4,\n",
        "    #             expand=2,\n",
        "    #             )\n",
        "\n",
        "    model = MambaLM(vocab_size, \n",
        "                n_layers=12,\n",
        "                n_heads=12,\n",
        "                d_model=768,\n",
        "                d_state=64,\n",
        "                d_conv=4,\n",
        "                expand=2,\n",
        "                )\n",
        "    \n",
        "    special_print(f\"Model params: {num_params(model)}\", accelerator)\n",
        "    special_print(f\"Token:params ratio: {num_tokens / num_params(model)}\", accelerator)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), betas=(0.9, 0.95), weight_decay=0.1, lr=0.001)\n",
        "    # optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
        "\n",
        "    model, dataloader, optimizer, scheduler = accelerator.prepare(model, dataloader, optimizer, scheduler)\n",
        "    special_print(\"Prepared with Accelerator\", accelerator)\n",
        "    model = torch.compile(model)\n",
        "    special_print(\"Model compiled\", accelerator)\n",
        "\n",
        "    epochs = 1\n",
        "    MambaLM.train(model, dataloader, optimizer, scheduler, epochs, accelerator)\n",
        "    \n",
        "    # 6. Save final model\n",
        "    # Must unwrap to gather full weights from all shards\n",
        "    unwrapped_model = accelerator.unwrap_model(model)\n",
        "    torch.save(unwrapped_model.state_dict(), \"my_fsdp_fp32_model.pt\")\n",
        "    print(\"Model saved at my_fsdp_fp32_model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "bfef06c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "if debug:\n",
        "    model = MambaLM(vocab_size, \n",
        "                    n_layers=12,\n",
        "                    n_heads=12,\n",
        "                    d_model=768,\n",
        "                    d_state=64,\n",
        "                    d_conv=4,\n",
        "                    expand=2,\n",
        "                ).to('cuda')\n",
        "    print(num_params(model))\n",
        "    model.generate_text(\"hello there, happy world! test lol lol\", 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "7c69d10b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Launching training on one GPU.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accelerator initialized\n",
            "Current Time = 23:29:11.895\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f91f5d6dbb64d9cb370191ecca0a844",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0185b4e042b04b0495c3bcfda1314b50",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/48 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "66acbfadff7c4f70bb5ccba022dd90cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading dataset shards:   0%|          | 0/48 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded dataloader with 1372250 batches\n",
            "Current Time = 23:29:48.946\n",
            "Number of tokens: 2810368000\n",
            "Model params: 131429089\n",
            "Current Time = 23:29:50.342\n",
            "Token:params ratio: 21.38315057483203\n",
            "Current Time = 23:29:50.344\n",
            "Prepared with Accelerator\n",
            "Current Time = 23:29:50.941\n",
            "Model compiled\n",
            "Current Time = 23:29:52.073\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a942b95fd91411a8a2d4e3d486a95ab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training (process 0):   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d09aed9e2ce5408aa704087e17e6af98",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches (process 0):   0%|          | 0/1372250 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test generation `the weather today is` at 0.00% completion:  the weather today is<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "Test generation `the weather today is` at 0.01% completion:  the weather today is and:::::::::\n"
          ]
        }
      ],
      "source": [
        "nb_parallelize = False\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Check if we're running in a notebook or regular Python script\n",
        "    if is_in_notebook():\n",
        "        if nb_parallelize:\n",
        "            notebook_launcher(training_function, num_processes=8)\n",
        "        else:\n",
        "            notebook_launcher(training_function, num_processes=1)\n",
        "    else:\n",
        "        training_function()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30989bd8",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Model params\", num_params(model))\n",
        "print(\"Layer params\", num_params(model.layers[0]))\n",
        "print(\"Head params\", num_params(model.layers[0].Heads[0]))\n",
        "print(\"SSM params\", num_params(model.layers[0].Heads[0].ssm))\n",
        "print(\"Conv params\", num_params(model.layers[0].Heads[0].conv))\n",
        "print(\"Up params\", num_params(model.layers[0].Heads[0].upscale))\n",
        "print(\"Gate params\", num_params(model.layers[0].Heads[0].gate))\n",
        "print(\"Down params\", num_params(model.layers[0].Heads[0].downscale))\n",
        "print(\"RMS params\", num_params(model.layers[0].rms_norm))\n",
        "print(\"Out params\", num_params(model.layers[0].out_project))\n",
        "print(\"Layer norm params\", num_params(model.layers[0].layer_norm))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
